{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604cfa05-a1ad-4f99-b547-8980c525fa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数量： 8785\n",
      "missing keys:[]\n",
      "unexpected keys:['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "error msgs:[]\n",
      "1.0213528871536255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/CCKS-2022/pretrain.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0my_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 第一步，对NEZHA模型进行继续预训练（MLM任务）。\n",
    "%run pretrain \\\n",
    "            --EPOCH 10 \\\n",
    "            --BATCH_SIZE 8 \\\n",
    "            --seed 1997 \\\n",
    "            --model_name \"nezha-base-cn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0caeda-90bb-4f70-9a21-cf0d72da5380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882\n",
      "783\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "step: 200, loss: 0.2664216223731637\n",
      "step: 400, loss: 0.23655616644769906\n",
      "step: 600, loss: 0.20870424332097173\n",
      "step: 800, loss: 0.1522583679575473\n",
      "第 1 轮训练结束\n",
      "acc: 0.8863345980644226, f1: 0.8589540123939514\n",
      "F1 macro: tensor(0.8590)\n",
      "882\n",
      "783\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "step: 200, loss: 0.2649458732455969\n",
      "step: 400, loss: 0.1755057533364743\n",
      "step: 600, loss: 0.1559830985404551\n",
      "step: 800, loss: 0.13849834583699702\n",
      "第 1 轮训练结束\n",
      "acc: 0.8505747318267822, f1: 0.8059701323509216\n",
      "F1 macro: tensor(0.8060)\n",
      "882\n",
      "783\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "step: 200, loss: 0.26620404951274396\n",
      "step: 400, loss: 0.1829696380533278\n",
      "step: 600, loss: 0.15623755260836333\n",
      "step: 800, loss: 0.13540672337869183\n",
      "第 1 轮训练结束\n",
      "acc: 0.8799489140510559, f1: 0.8368055820465088\n",
      "F1 macro: tensor(0.8368)\n",
      "882\n",
      "783\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "step: 200, loss: 0.2664894368685782\n",
      "step: 400, loss: 0.23594671908766032\n",
      "step: 600, loss: 0.20501581816002726\n",
      "step: 800, loss: 0.14770040263421833\n",
      "第 1 轮训练结束\n",
      "acc: 0.8722860813140869, f1: 0.8257839679718018\n",
      "F1 macro: tensor(0.8258)\n",
      "训练结束\n"
     ]
    }
   ],
   "source": [
    "# 第二步，进行FineTune。加入统计信息，训练NEZHA模型，数据为10折中的第6,7,8,9折。其他参数如下所示。\n",
    "%run train.py \\\n",
    "            --BATCH_SIZE 8 \\\n",
    "            --EPOCH 1 \\\n",
    "            --DEVICE 'cuda:0' \\\n",
    "            --model_name \"nezha-base-cn\" \\\n",
    "            --k_fold 10 \\\n",
    "            --seed 1997 \\\n",
    "            --lr 2e-5 \\\n",
    "            --fold_list 6 7 8 9 \\\n",
    "            --input_format \"statistics+320+64\" \\\n",
    "            --output_prefix \"model_statistics_nezha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b469ed4f-8767-4e8f-80b6-226685ffdfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882\n",
      "784\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "step: 200, loss: 0.2541269680391997\n",
      "step: 400, loss: 0.1503575404919684\n",
      "step: 600, loss: 0.15423430075868963\n",
      "step: 800, loss: 0.14957240639720112\n",
      "第 1 轮训练结束\n",
      "acc: 0.8826530575752258, f1: 0.836298942565918\n",
      "F1 macro: tensor(0.8363)\n",
      "训练结束\n"
     ]
    }
   ],
   "source": [
    "# 第三步，进行FineTune。加入统计信息，训练NEZHA模型，不分k折，进行所有数据的finetune。其他参数如下所示。\n",
    "%run train.py \\\n",
    "            --BATCH_SIZE 8 \\\n",
    "            --EPOCH 1 \\\n",
    "            --DEVICE 'cuda:0' \\\n",
    "            --model_name \"nezha-base-cn\" \\\n",
    "            --seed 1997 \\\n",
    "            --lr 2e-5 \\\n",
    "            --all_data True \\\n",
    "            --input_format \"statistics+320+64\" \\\n",
    "            --output_prefix \"model_statistics_nezha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bae4d8-8c3a-4777-87e5-f53d4ed7856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861\n",
      "392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-xlnet-base were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-xlnet-base and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200, loss: 0.2685621193423867\n",
      "step: 400, loss: 0.21028759784530848\n",
      "step: 600, loss: 0.22043054725974798\n",
      "step: 800, loss: 0.16992886482272296\n",
      "step: 1000, loss: 0.20479903370141983\n",
      "step: 1200, loss: 0.1732196862087585\n",
      "step: 1400, loss: 0.15525632159085945\n",
      "step: 1600, loss: 0.16218716703297104\n",
      "step: 1800, loss: 0.15752691822126508\n",
      "第 1 轮训练结束\n",
      "acc: 0.8545918464660645, f1: 0.8093645572662354\n",
      "F1 macro: tensor(0.8094)\n",
      "1861\n",
      "392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-xlnet-base were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-xlnet-base and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200, loss: 0.25848889604210856\n",
      "step: 400, loss: 0.20271643431624398\n",
      "step: 600, loss: 0.20963420788757503\n",
      "step: 800, loss: 0.181042528396938\n",
      "step: 1000, loss: 0.17919637804618105\n",
      "step: 1200, loss: 0.18480390775948763\n",
      "step: 1400, loss: 0.15258885548682882\n",
      "step: 1600, loss: 0.15618000855727587\n",
      "step: 1800, loss: 0.16402553376799914\n",
      "第 1 轮训练结束\n",
      "acc: 0.8545918464660645, f1: 0.8155339956283569\n",
      "F1 macro: tensor(0.8155)\n",
      "训练结束\n"
     ]
    }
   ],
   "source": [
    "# 第四步，进行FineTune。加入统计信息，训练XLNET模型，数据为10折模型的第0、1折。其他参数如下所示。（由于机器显存不足所以batchsize为4。若觉得太慢，可增大batch_size，但记得同步调整learning rate）\n",
    "%run train.py \\\n",
    "            --BATCH_SIZE 4 \\\n",
    "            --EPOCH 1 \\\n",
    "            --DEVICE 'cuda:0' \\\n",
    "            --model_name \"xlnet-base-cn\" \\\n",
    "            --k_fold 20 \\\n",
    "            --seed 1997 \\\n",
    "            --lr 1e-5 \\\n",
    "            --max_seq_len 1024 \\\n",
    "            --fold_list 0 1 \\\n",
    "            --input_format \"statistics+512+128\" \\\n",
    "            --output_prefix \"model_statistics_xlnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba402835-55d3-4048-b103-34799d1fef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931\n",
      "392\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "step: 200, loss: 0.10923498359508813\n",
      "step: 400, loss: 0.08866714839823545\n",
      "step: 600, loss: 0.08533922892063855\n",
      "step: 800, loss: 0.07614429018460214\n",
      "第 1 轮训练结束\n",
      "acc: 0.8622449040412903, f1: 0.8163265585899353\n",
      "F1 macro: tensor(0.8163)\n",
      "931\n",
      "392\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "step: 200, loss: 0.11122305232100188\n",
      "step: 400, loss: 0.08977031848393381\n",
      "step: 600, loss: 0.08710013125091791\n",
      "step: 800, loss: 0.0728524409327656\n",
      "第 1 轮训练结束\n",
      "acc: 0.8801020383834839, f1: 0.8417508602142334\n",
      "F1 macro: tensor(0.8418)\n",
      "931\n",
      "392\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "step: 200, loss: 0.11109291536733508\n",
      "step: 400, loss: 0.09430916094221174\n",
      "step: 600, loss: 0.08761758723761887\n",
      "step: 800, loss: 0.07707778621930629\n",
      "第 1 轮训练结束\n",
      "acc: 0.8877550959587097, f1: 0.8533333539962769\n",
      "F1 macro: tensor(0.8533)\n",
      "931\n",
      "392\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "step: 200, loss: 0.11118435623124241\n",
      "step: 400, loss: 0.09042544367723167\n",
      "step: 600, loss: 0.0876896009594202\n",
      "step: 800, loss: 0.07615309177199378\n",
      "第 1 轮训练结束\n",
      "acc: 0.8775510191917419, f1: 0.8139534592628479\n",
      "F1 macro: tensor(0.8140)\n",
      "训练结束\n"
     ]
    }
   ],
   "source": [
    "# 第五步，进行FineTune。加入统计信息，训练NEZHA模型，数据为10折模型的第0、1、2、3折。其他参数如下所示。\n",
    "%run train.py \\\n",
    "            --BATCH_SIZE 8 \\\n",
    "            --EPOCH 1 \\\n",
    "            --DEVICE 'cuda:0' \\\n",
    "            --model_name \"nezha-base-cn\" \\\n",
    "            --k_fold 20 \\\n",
    "            --seed 1997 \\\n",
    "            --lr 2e-5 \\\n",
    "            --gamma 2 \\\n",
    "            --is_fgm True \\\n",
    "            --fold_list 0 1 2 3 \\\n",
    "            --input_format \"200+entities\" \\\n",
    "            --output_prefix \"model_nezha_entities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd561811-7736-4ed3-88b0-d67b63c48deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "600 506\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "377 729\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "539 567\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "661 445\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "430 676\n"
     ]
    }
   ],
   "source": [
    "# 第六步，根据训练好的11个模型进行预测，预测出11份结果。本部分是预测部分的第一部分。\n",
    "%run predict.py \\\n",
    "            --BATCH_SIZE 8 \\\n",
    "            --DEVICE 'cuda:0' \\\n",
    "            --model_name \"nezha-base-cn\" \\\n",
    "            --model_list \"trained_model/model_statistics_nezha_6.pth\" \"trained_model/model_statistics_nezha_7.pth\" \\\n",
    "                        \"trained_model/model_statistics_nezha_8.pth\" \"trained_model/model_statistics_nezha_9.pth\" \"trained_model/model_statistics_nezha_all.pth\" \\\n",
    "            --input_format \"statistics+320+64\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e3771c-f515-4a99-8559-c19d92562c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-xlnet-base were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-xlnet-base and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642 464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-xlnet-base were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-xlnet-base and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735 371\n"
     ]
    }
   ],
   "source": [
    "# 第七步，根据训练好的11个模型进行预测，预测出11份结果。本部分是预测部分的第二部分。\n",
    "%run predict.py \\\n",
    "            --BATCH_SIZE 8 \\\n",
    "            --DEVICE 'cuda:0' \\\n",
    "            --model_name \"xlnet-base-cn\" \\\n",
    "            --model_list \"trained_model/model_statistics_xlnet_0.pth\" \"trained_model/model_statistics_xlnet_1.pth\" \\\n",
    "            --input_format \"statistics+768+128\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7638ad4f-f2c1-410f-9fac-1a412fff940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "697 409\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "719 387\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "709 397\n",
      "missing keys:[]\n",
      "unexpected keys:[]\n",
      "error msgs:[]\n",
      "727 379\n"
     ]
    }
   ],
   "source": [
    "# 第八步，根据训练好的11个模型进行预测，预测出11份结果。本部分是预测部分的第三部分。\n",
    "%run predict.py \\\n",
    "            --BATCH_SIZE 8 \\\n",
    "            --DEVICE 'cuda:0' \\\n",
    "            --model_name \"nezha-base-cn\" \\\n",
    "            --model_list \"trained_model/model_nezha_entities_0.pth\" \"trained_model/model_nezha_entities_1.pth\" \\\n",
    "                    \"trained_model/model_nezha_entities_2.pth\" \"trained_model/model_nezha_entities_3.pth\" \\\n",
    "            --input_format \"200+entities\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102b0469-b38b-47a7-8371-b879b60fbc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参与投票的文件个数为：11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/CCKS-2022/voting.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_column[new_column <= zero_up] = 0\n",
      "/root/CCKS-2022/voting.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_column[new_column >= one_bottom] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果中0的数量为：673\n",
      "预测结果中1的数量为：433\n",
      "预测结果1的数量占总数据量的占比为：39.150090415913205%\n"
     ]
    }
   ],
   "source": [
    "# 第九步，根据上述11份结果进行Voting融合。\n",
    "%run voting.py \\\n",
    "            --dir_name \"output\" \\\n",
    "            --is_filter True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
